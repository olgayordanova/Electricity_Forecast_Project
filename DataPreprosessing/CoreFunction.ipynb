{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# import xgboost as xgb\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from functools import wraps\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Декоратор измерване на време за работа на функция\n",
    "def timeit(func):\n",
    "    @wraps(func)\n",
    "    def timeit_wrapper(*args, **kwargs):\n",
    "        start_time = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        total_time = end_time - start_time\n",
    "        print(f'Function took {total_time:.4f} seconds')\n",
    "        return result\n",
    "    return timeit_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Четене на данни\n",
    "def read_data(col_name, path):\n",
    "    df_data = pd.read_csv(path, sep=',') #, index_col =\"Unnamed: 0\"\n",
    "    target_cols = [col for col in df_data.columns if 'Target' in col]\n",
    "    target = df_data[target_cols]       \n",
    "    attribute = df_data.drop([\"time\", col_name], axis = 1)\n",
    "    attribute = attribute.drop(target_cols, axis = 1)\n",
    "    return target, attribute, df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скалиране на данни\n",
    "def scaled_data(target, attribute, ts, scaler): \n",
    "    attribute_col_names = attribute.columns\n",
    "    attribute = scaler.fit_transform(attribute)\n",
    "    atribute_train, atribute_test, target_train, target_test = train_test_split( \n",
    "    attribute, target, test_size=ts, random_state=42)\n",
    "    \n",
    "    return atribute_train, atribute_test, target_train, target_test, attribute_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запазване на модел във файл\n",
    "def save_model(model, file_name):\n",
    "    with open( os.path.join(r'saved_models/', file_name), 'wb') as file:\n",
    "        pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зареждане на модел от файл\n",
    "def load_model(file_name):\n",
    "    with open( os.path.join(r'saved_models/', file_name), 'rb') as file:\n",
    "        load_model = pickle.load(file)\n",
    "    return load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Плотва четирите графики за конкретен модел - прогнозирани стойности срещу наблюдавани\n",
    "# Вика от тялото си plot_charts()\n",
    "def get_and_plot_data(prediction, target_test, start_index, stop_index):\n",
    "    \n",
    "    for i in range (start_index, stop_index):\n",
    "        e_minus_o = pd.DataFrame()\n",
    "        df1 = pd.DataFrame(prediction[:, i])\n",
    "        df2 = pd.DataFrame(target_test[f\"Target_{i+1}\"])\n",
    "        e_minus_o = pd.concat([df1, df2.set_index(df1.index)], axis=1)\n",
    "        e_minus_o[\"diff\"] = e_minus_o[f\"Target_{i+1}\"]-e_minus_o[0]\n",
    "        e_minus_o.columns = [\"Actual\", f\"Predicted_Target_{i+1}\", \"Predicted-Actual\"]\n",
    "\n",
    "        plot_charts(e_minus_o, i)\n",
    "\n",
    "    return e_minus_o\n",
    "\n",
    "# 1. хистограма обща на - Predicted и Actual; \n",
    "# 2. Resudual plot - x-actual y-predicted] \n",
    "# 3.хистограма на - Residual = Predicted-Actual \n",
    "# 4. scaterplot - x- predicted  y resudual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ползва се за get_and_plot_data()\n",
    "def plot_charts(df, i):\n",
    "    title_fondsize = 16\n",
    "    f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20, 5)) \n",
    "    mu = df[\"Predicted-Actual\"].mean()\n",
    "    sigma = df[\"Predicted-Actual\"].std()\n",
    "    textstr = '\\n'.join((\n",
    "        r'$\\mu=%.2f$' % (mu, ),\n",
    "        r'$\\sigma=%.2f$' % (sigma, ))) \n",
    "    mu_predicted_a = df[\"Actual\"].mean()\n",
    "    mu_predicted_t = df[f\"Predicted_Target_{i+1}\"].mean()\n",
    "    textstr_ax1 = '\\n'.join((\n",
    "        r'$\\mu_{actual}=%.2f$' % (mu_predicted_a, ),\n",
    "        r'$\\mu_{predicted}=%.2f$' % (mu_predicted_t, ))) \n",
    "\n",
    "\n",
    "    ax1.hist(df[\"Actual\"], color=\"darkgreen\", bins = \"fd\", alpha = 0.3, label='Actual')\n",
    "    ax1.hist(df[f\"Predicted_Target_{i+1}\"], color=\"midnightblue\", bins = \"fd\", alpha = 0.5, label='Predicted')\n",
    "    ax1.axvline(df[\"Actual\"].mean(), color='darkgreen', linestyle='dashed', linewidth=3)\n",
    "    ax1.axvline(df[f\"Predicted_Target_{i+1}\"].mean(), color='midnightblue', linestyle='dashed', linewidth=1)    \n",
    "    ax1.set_title(f'Histograms Actual vs Predicted for Target_{i+1}', fontsize=title_fondsize)\n",
    "    ax1.text(0.05, 0.95, textstr_ax1, transform=ax1.transAxes, fontsize=14, verticalalignment='top', horizontalalignment='left')\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel(f\"Actual  vs Predicted for Target_{i+1}\") \n",
    "    ax1.set_ylabel(\"Density\")\n",
    "\n",
    "    \n",
    "    ax2.scatter( df[\"Actual\"], df[f\"Predicted_Target_{i+1}\"], color=\"darkgreen\", s=1, alpha = 0.5)\n",
    "    ax2.set_xlabel(\"Actual\")\n",
    "    ax2.set_ylabel(f\"Predicted Price for Target_{i+1}\")\n",
    "    ax2.set_title(f'Actual  vs Predicted for Target_{i+1}', fontsize=title_fondsize)\n",
    "    \n",
    "    ax3.scatter( df[f\"Predicted_Target_{i+1}\"], df[\"Predicted-Actual\"], color=\"midnightblue\", alpha = 0.5, s=1)\n",
    "    ax3.set_xlabel(\"Predicted\")\n",
    "    ax3.set_ylabel(\"Residuals\")\n",
    "    ax3.set_title('Residual vs Predicted', fontsize=title_fondsize)\n",
    "\n",
    "\n",
    "    ax4.hist(df[\"Predicted-Actual\"], color=\"crimson\", bins = \"fd\", alpha = 0.8)\n",
    "    ax4.axvline(mu, color='midnightblue', linestyle='dashed', linewidth=1)\n",
    "    ax4.text(0.05, 0.95, textstr, transform=ax4.transAxes, fontsize=14, verticalalignment='top', horizontalalignment='left')\n",
    "    ax4.set_xlabel(f\"Residuals\") \n",
    "    ax4.set_ylabel(\"Density\")\n",
    "    ax4.set_title('Residual plot', fontsize=title_fondsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_score_distributions(model, atribute_train, target_train, atribute_test, target_test, bplot=True):\n",
    "    r2_train = r2_score(model.predict(atribute_train), target_train, multioutput='raw_values')\n",
    "    r2_test =  r2_score(model.predict(atribute_test), target_test, multioutput='raw_values')\n",
    "\n",
    "    mae_train = mean_absolute_error(model.predict(atribute_train), target_train, multioutput='raw_values')\n",
    "    mae_test =  mean_absolute_error(model.predict(atribute_test), target_test, multioutput='raw_values')\n",
    "\n",
    "    mse_train = mean_squared_error(model.predict(atribute_train), target_train, multioutput='raw_values')\n",
    "    mse_test =  mean_squared_error(model.predict(atribute_test), target_test, multioutput='raw_values')\n",
    "    \n",
    "    if bplot:\n",
    "        plot_scores(r2_train, r2_test, mae_train, mae_test, mse_train, mse_test)\n",
    "    \n",
    "    return r2_train, r2_test, mae_train, mae_test, mse_train, mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(r2_train, r2_test, mae_train, mae_test, mse_train, mse_test):\n",
    "    title_fondsize = 14\n",
    "    f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5)) \n",
    "\n",
    "    ax1.hist(r2_train, color=\"darkgreen\", bins = \"fd\", alpha = 0.5, label='Train R2')\n",
    "    ax1.hist(r2_test, color=\"midnightblue\", bins = \"fd\", alpha = 0.5, label='Test R2')\n",
    "    ax1.set_title(\"Coefficient of Determination Distribution\")\n",
    "    ax1.set_xlabel(\"R2\") \n",
    "    ax1.set_ylabel(\"Count\")\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.hist(mae_train, color=\"darkgreen\", bins = \"fd\", alpha = 0.5, label='Train MAE')\n",
    "    ax2.hist(mae_test, color=\"midnightblue\", bins = \"fd\", alpha = 0.5, label='Test MAE')\n",
    "    ax2.set_title(\"Mean Absolute Error Distribution\")\n",
    "    ax2.set_xlabel(\"MAE\") \n",
    "    ax2.set_ylabel(\"Count\")\n",
    "    ax2.legend()\n",
    "\n",
    "    ax3.hist(mse_train, color=\"darkgreen\", bins = \"fd\", alpha = 0.5, label='Train MSE')\n",
    "    ax3.hist(mse_test, color=\"midnightblue\", bins = \"fd\", alpha = 0.5, label='Test MSE')\n",
    "    ax3.set_title(\"Mean Squared Error Distribution\")\n",
    "    ax3.set_xlabel(\"MSE\") \n",
    "    ax3.set_ylabel(\"Count\")\n",
    "    ax3.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ползва се за get_ttest() и get_ztest ()\n",
    "def composite_dataframe(price_prediction, price_target_test, idx):\n",
    "    e_minus_o = pd.DataFrame()\n",
    "    df1 = pd.DataFrame(price_prediction[:, idx])\n",
    "    df2 = pd.DataFrame(price_target_test[f\"Target_{idx+1}\"])\n",
    "    e_minus_o = pd.concat([df1, df2.set_index(df1.index)], axis=1)\n",
    "    e_minus_o[\"diff\"] = e_minus_o[f\"Target_{idx+1}\"]-e_minus_o[0]\n",
    "    e_minus_o.columns = [\"Actual\", f\"Predicted_Target_{idx+1}\", \"Predicted-Actual\"]\n",
    "    return e_minus_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-test normality\n",
    "def get_ttest(price_prediction, price_target_test, idx):\n",
    "    df = composite_dataframe(price_prediction, price_target_test, idx)\n",
    "    str = \"\"\n",
    "    tset, pval = ttest_1samp(df[\"Predicted-Actual\"], 0)\n",
    "    str = str + f\"p-value: {pval}\\n\"\n",
    "\n",
    "    if pval < 0.05:    # alpha value is 0.05 or 5%\n",
    "        str = str + \"we are rejecting null hypothesis\"\n",
    "    else:\n",
    "        str = str + \"we are accepting null hypothesis\" #\"fail to reject the null hypothesis\"\n",
    "    return str\n",
    "# The two hypotheses for this particular two sample t-test are as follows:\n",
    "    # H0: µ1 = 0 \n",
    "    # HA: µ1 ≠ 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ttest_2samples(sample_train, sample_test, bprint = True):\n",
    "    str = \"\"\n",
    "    tset, pval = ttest_ind(sample_train, sample_test, equal_var=True)\n",
    "    \n",
    "    str = str + f\"p-value: {pval}\\n\"\n",
    "\n",
    "    if pval < 0.05:    # alpha value is 0.05 or 5%\n",
    "        str = str + \"we are rejecting null hypothesis\"\n",
    "    else:\n",
    "        str = str + \"fail to reject the null hypothesis\"\n",
    "    \n",
    "    if bprint:\n",
    "        print(str)\n",
    "    return tset, pval\n",
    "\n",
    "# The two hypotheses for this particular two sample t-test are as follows:\n",
    "    # H0: µ1 = µ2 (the two population means are equal)\n",
    "    # HA: µ1 ≠µ2 (the two population means are not equal)\n",
    "# Because the p-value of our test (0.53005) is greater than alpha = 0.05, we fail to reject the null hypothesis of the test. \n",
    "# We do not have sufficient evidence to say that the mean height of plants between the two populations is different.\n",
    "# Нулевата хипотеза гласи, че няма никаква статистическа значимост между двете средни стойности на съвкупността (H0),  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-test normality\n",
    "def get_ztest (price_prediction, price_target_test, idx):\n",
    "    df = composite_dataframe(price_prediction, price_target_test, idx)\n",
    "    str = \"\"\n",
    "    ztest, pval = stests.ztest(df[\"Predicted-Actual\"], x2=None, value=0)\n",
    "    str = str + f\"p-value: {pval}\\n\"\n",
    "\n",
    "    if pval < 0.05:    # alpha value is 0.05 or 5%\n",
    "        str = str + \"reject null hypothesis\"\n",
    "    else:\n",
    "        str = str + \"accept null hypothesis\" #\"fail to reject the null hypothesis\"\n",
    "    return str\n",
    "\n",
    "# A two sample z-test uses the following null and alternative hypotheses:    \n",
    "    # H0: µ1 = 0 \n",
    "    # HA: µ1 ≠ 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ztest_2samples(sample_train, sample_test, bprint = True):\n",
    "    str = \"\"\n",
    "    ztest, pval = stests.ztest(sample_train, sample_test, value=0)\n",
    "    \n",
    "    str = str + f\"p-value: {pval}\\n\"\n",
    "\n",
    "    if pval < 0.05:    # alpha value is 0.05 or 5%\n",
    "        str = str + \"reject null hypothesis\"\n",
    "    else:\n",
    "        str = str + \"fail to reject the null hypothesis\"    \n",
    "    \n",
    "    if bprint:\n",
    "        print(str)    \n",
    "    return ztest, pval\n",
    "\n",
    "# A two sample z-test uses the following null and alternative hypotheses:\n",
    "\n",
    "#     H0: μ1 = μ2 (the two population means are equal)\n",
    "#     HA: μ1 ≠ μ2 (the two population means are not equal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ползва при анализа на данните - за dimentionality reduction\n",
    "def get_feature_importance_data(df, col_name):\n",
    "    data = df.copy()\n",
    "    y = data[col_name]\n",
    "    data = data.drop(col_name, axis=1)\n",
    "    X = data.iloc[:, 1:]\n",
    "    X = (X - X.min ()) / (X.max () - X.min ())\n",
    "   \n",
    "    train_samples = int(X.shape[0] * (1-TEST_SIZE ))\n",
    " \n",
    "    X_train = X.iloc[:train_samples]\n",
    "    X_test = X.iloc[train_samples:]\n",
    "\n",
    "    y_train = y.iloc[:train_samples]\n",
    "    y_test = y.iloc[train_samples:]\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA data preprocessing\n",
    "# n_comp = 0.95 ---> explained variance ratio - запазваме 95% от вариацията на данните\n",
    "@timeit\n",
    "def PCA_feature_preprocessing(atribute_train, atribute_test, n_comp = 0.95, col_names = []):\n",
    "    \n",
    "    pca = PCA(n_components=n_comp)\n",
    "    PC_train = pca.fit_transform(atribute_train)\n",
    "    PC_test = pca.transform(atribute_test)\n",
    "    \n",
    "    n_pcs= pca.components_.shape[0]\n",
    "    initial_feature_names = col_names #atribute_train.columns\n",
    "    most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "    most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "    draw_cut_off_threshold(most_important_names, pca, n_comp)\n",
    "\n",
    "    print(\"Transformed train shape:\", PC_train.shape)\n",
    "    print(\"Transformed test shape:\", PC_test.shape)\n",
    "\n",
    "    return most_important_names, PC_train, PC_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA data preprocessing visualization\n",
    "def draw_cut_off_threshold (most_important_names, pca, n_comp = 0.90): \n",
    "    plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    xi = np.arange(1, len(most_important_names)+1, step=1)\n",
    "    y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    plt.ylim(0.0,1.1)\n",
    "    plt.plot(xi, y, marker='o', linestyle='--', color='darkgreen')\n",
    "\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.xticks(np.arange(0, len(most_important_names)+2, step=10))\n",
    "    plt.ylabel('Cumulative variance (%)')\n",
    "    plt.title('The number of components needed to explain variance')\n",
    "\n",
    "    plt.axhline(y = n_comp, color='midnightblue', linestyle='-')\n",
    "    plt.text(0.2, 0.85, f'{n_comp*100}% cut-off threshold', color = 'midnightblue', fontsize=16)\n",
    "    ax.grid(axis='x')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Плотва прогнозирани стойности срещу наблюдавани за всяко едно наблюдение\n",
    "def plot_actual_vs_prediction (prediction, target_test, variable_name, start_inx = 0, end_inx = 1):    \n",
    "    for i in range (start_inx, end_inx):\n",
    "        df1 = pd.DataFrame(prediction[i, :])\n",
    "        df2 = pd.DataFrame(target_test.iloc[i, :])\n",
    "        df_char = pd.concat([df1, df2.set_index(df1.index)], axis=1)\n",
    "        df_char.columns = [\"Predicted\", \"Actual\"]       \n",
    "        sigma = df_char[\"Predicted\"].std()\n",
    "        df_char[\"Predicted + sigma\"] = df_char[\"Predicted\"]+sigma\n",
    "        df_char[\"Predicted - sigma\"] = df_char[\"Predicted\"]-sigma\n",
    "        \n",
    "        lablestr1 = r'Predicted + $\\sigma (%.2f)$' % (sigma, )\n",
    "        lablestr2 = r'Predicted - $\\sigma (%.2f)$' % (sigma, )\n",
    "       \n",
    "        plt.figure(figsize=(14, 3), dpi=100)\n",
    "        plt.plot(df_char.index, df_char['Actual'], label=f'Actual {variable_name}', color='darkgreen', alpha = 0.7) \n",
    "        plt.plot(df_char.index, df_char['Predicted'], label=f'Predicted {variable_name}',color='midnightblue', alpha = 0.7)\n",
    "        plt.plot(df_char.index, df_char['Predicted + sigma'], label=lablestr1, color='lightslategray', alpha = 0.5, linestyle='dashed', linewidth=1)\n",
    "        plt.plot(df_char.index, df_char['Predicted - sigma'], label=lablestr2, color='lightslategray', alpha = 0.5, linestyle='dashed', linewidth=1)\n",
    "        plt.fill_between(df_char.index, df_char['Predicted - sigma'], df_char['Predicted + sigma'], color='lightslategray', alpha=0.15)        \n",
    "        plt.title(f'Predicted vs Actual for observation {i}', fontsize=14)\n",
    "        plt.xlabel('Hours ahead')\n",
    "        plt.ylabel(variable_name)\n",
    "        plt.legend(loc='upper left')\n",
    "    #     plt.savefig(f\"pictures/company_base_model.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Плотва разпределенията на стойността на зависимата променлива от остатъка - за таргет по избор\n",
    "def plot_feature_vs_residuals_PCA(att, prediction, target_test, start_index=0, stop_index=1, target_indx =0, col_name = None): #0 to count(features)   \n",
    "    df = pd.DataFrame(att)\n",
    "    att_col_names = [f\"PCA_{j}\" for j in range(0, att.shape[1])]\n",
    "    df.columns = att_col_names\n",
    "\n",
    "    e_minus_o = composite_dataframe(prediction, target_test, idx=target_indx)\n",
    "    df['Residuals'] = e_minus_o['Predicted-Actual']\n",
    "    if col_name:\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.scatter(df[col_name], df[\"Residuals\"], color=\"midnightblue\", s=2, alpha = 0.5)\n",
    "        plt.xlabel(col_name)\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        plt.title(f'Feature {col_name} vs Residuals', fontsize=14)\n",
    "    else:\n",
    "        for i in range (start_index, stop_index):\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            index = att_col_names.index(att_col_names[i])\n",
    "            plt.scatter(df.iloc[:, index], df[\"Residuals\"], color=\"midnightblue\", s=2, alpha = 0.5)                      \n",
    "            plt.title(f'Feature {att_col_names[i]} vs Residuals', fontsize=14)\n",
    "            plt.xlabel(f\"{att_col_names[i]}\") \n",
    "            plt.ylabel(\"Residuals\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Плотва разпределенията на стойността на зависимата променлива от остатъка - за таргет по избор\n",
    "def plot_feature_vs_residuals(att, att_col_names, prediction, target_test, start_index=0, stop_index=1, target_indx =0, col_name = None): #0 to count(features)\n",
    "         \n",
    "    df = pd.DataFrame(att)\n",
    "    df.columns = att_col_names\n",
    "    e_minus_o = composite_dataframe(prediction, target_test, idx=target_indx)\n",
    "    df['Residuals'] = e_minus_o['Predicted-Actual']\n",
    "    if col_name:\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.scatter(df[col_name], df[\"Residuals\"], color=\"midnightblue\", s=2, alpha = 0.5)\n",
    "        plt.xlabel(col_name)\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        plt.title(f'Feature {col_name} vs Residuals', fontsize=14)\n",
    "    else:\n",
    "        for i in range (start_index, stop_index):\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            plt.scatter(df[att_col_names[i]], df[\"Residuals\"], color=\"midnightblue\", s=2, alpha = 0.5)                                 \n",
    "            plt.title(f'Feature {att_col_names[i]} vs Residuals', fontsize=14)\n",
    "            plt.xlabel(f\"{att_col_names[i]}\") \n",
    "            plt.ylabel(\"Residuals\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    prediction, target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# изчислява и отпечатва резултат от кросвалидация на модел\n",
    "@timeit\n",
    "def cv_score(model, trian_atribute, train_target):\n",
    "    \n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "    mae = cross_val_score(model, trian_atribute, train_target, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "    mse = cross_val_score(model, trian_atribute, train_target, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "    rmse = cross_val_score(model, trian_atribute, train_target, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "    r2 = cross_val_score(model, trian_atribute, train_target, scoring='r2', cv=cv, n_jobs=-1)\n",
    "    \n",
    "    mae = np.absolute(mae)\n",
    "    mse = np.absolute(mse)\n",
    "    rmse = np.absolute(rmse)\n",
    "    r2 = np.absolute(r2)\n",
    "    \n",
    "    print('Cross validated Mean Squared Error: %.3f (%.3f)' % (np.mean(mse), np.std(mse)))\n",
    "    print('Cross validated Root Mean Squared Error: %.3f (%.3f)' % (np.mean(rmse), np.std(rmse)))\n",
    "    print('Cross validated Mean Absolute Error: %.3f (%.3f)' % (np.mean(mae), np.std(mae)))    \n",
    "    print('Cross validated Coefficient of Determination: %.3f (%.3f)' % (np.mean(r2), np.std(r2)))\n",
    "    \n",
    "    return np.mean(mse), np.mean(rmse), np.mean(mae), np.mean(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# изчислява и отпечатва резултат от обучение на модел\n",
    "def evaluation_report(model, atribute, target):\n",
    "    \n",
    "    prediction = model.predict(atribute)\n",
    "    mse = mean_squared_error(target, prediction)\n",
    "    rmse = mean_squared_error(target, prediction, squared=False)\n",
    "    mae = mean_absolute_error(target, prediction)\n",
    "    r2 = r2_score(target, prediction)\n",
    "    \n",
    "    print(\"Score Results:\")\n",
    "    print(\"Mean squared error: %.2f\" % mse)\n",
    "    print(\"Root mean squared error: %.2f\" % rmse)\n",
    "    print(\"Mean absolute error: %.2f\" % mae)\n",
    "    print(\"Coefficient of determination: %.4f\" % r2)\n",
    "    print('Predicted mean: %.3f (%.3f)' % (np.mean(prediction), np.std(prediction)))\n",
    "#     print('Actual mean: %.3f (%.3f)' % (np.mean(target), np.std(target)))\n",
    "    \n",
    "    return mse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# изчислява и връща резултат от обучение на модел\n",
    "def evaluation_results(model, atribute, target):\n",
    "    \n",
    "    prediction = model.predict(atribute)\n",
    "    mse = mean_squared_error(target, prediction)\n",
    "    rmse = mean_squared_error(target, prediction, squared=False)\n",
    "    mae = mean_absolute_error(target, prediction)\n",
    "    r2 = r2_score(target, prediction)\n",
    "    \n",
    "    return mse, mae, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV за подбор на хиперпараметри на модел\n",
    "@timeit\n",
    "def make_grid(estimator, params, cv, atribute, target):\n",
    "\n",
    "    grid = GridSearchCV(estimator=estimator, param_grid=params, cv=cv, return_train_score=True, n_jobs=-1)\n",
    "    grid.fit(atribute, target)\n",
    "    prediction = grid.predict(atribute)  \n",
    "    \n",
    "    print('Grid Results:')     \n",
    "    evaluation_report(gred.best_estimator_, atribute, target)\n",
    "    print(f'Best grid params: {grid.best_params_}')    \n",
    "   \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV за подбор на хиперпараметри на модел\n",
    "@timeit\n",
    "def get_grid_search(estimator,  params, cv, atribute_train, target_train):\n",
    "    grid_search = GridSearchCV(estimator=estimator,  param_grid=params, scoring='neg_mean_absolute_error',cv=cv, return_train_score=True, n_jobs=-1)\n",
    "    grid_search.fit(atribute_train, target_train)\n",
    "    print(grid_search.best_estimator_)\n",
    "    \n",
    "    return grid_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV за подбор на хиперпараметри на модел\n",
    "@timeit\n",
    "def make_random_grid(estimator, params, cv, atribute_train, target_train, n_iter=100):\n",
    "\n",
    "    grid = RandomizedSearchCV(estimator, param_distributions=params, random_state=42, n_iter=n_iter, cv=cv, verbose=1, n_jobs=-1, return_train_score=True)\n",
    "    grid.fit(atribute_train, target_train)\n",
    "    \n",
    "    print(grid.best_estimator_)    \n",
    " \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерира DataFrame с резултатите от всички модели\n",
    "def evaluation_report_df(model, atribute_train, target_train, atribute_test, target_test, model_filename, tranformed_data = False, wrapper = None):\n",
    "    \n",
    "    model_name = type(model).__name__\n",
    "    ddata = ''\n",
    "    if wrapper: \n",
    "        try:\n",
    "            model_name +=f\"_{type(model.estimator).__name__}\"\n",
    "            try:\n",
    "                model_name +=f\"_{type(model.estimator.base_estimator).__name__}\"\n",
    "            except:\n",
    "                pass           \n",
    "        except:\n",
    "            model_name +=f\"_{type(model.base_estimator).__name__}\"\n",
    "         \n",
    "    \n",
    "    if tranformed_data == False:\n",
    "        model_name+=\"_full_data\"\n",
    "        ddata+=\"_full_data\"\n",
    "    else:\n",
    "        model_name+=\"_PCA_data\"\n",
    "        ddata+=\"_PCA_data\"\n",
    "    \n",
    "        \n",
    "    \n",
    "    results = pd.DataFrame(columns=['Model','Data', 'mse_train', 'mae_train', 'rmse_train', 'r2_train', 'predicted_train_mean', \n",
    "                                    'predicted_train_std', 'mse_test', 'mae_test', 'rmse_test', 'r2_test', \n",
    "                                    'predicted_test_mean', 'predicted_test_std', 'pval_ztest_r2','pval_ttest_r2', 'pval_ztest_mae', 'pval_ttest_mae', 'model_filename'])\n",
    "    \n",
    "    prediction_train = model.predict(atribute_train)\n",
    "    prediction_test = model.predict(atribute_test)\n",
    "    \n",
    "    mse_train = mean_squared_error(target_train, prediction_train)\n",
    "    rmse_train = mean_squared_error(target_train, prediction_train, squared=False)\n",
    "    mae_train = mean_absolute_error(target_train, prediction_train)\n",
    "    r2_train = r2_score(target_train, prediction_train)\n",
    "    predicted_mean_train = np.mean(prediction_train)\n",
    "    predicted_std_train = np.std(prediction_train)\n",
    "\n",
    "    mse_test = mean_squared_error(target_test, prediction_test)\n",
    "    rmse_test = mean_squared_error(target_test, prediction_test, squared=False)\n",
    "    mae_test = mean_absolute_error(target_test, prediction_test)\n",
    "    r2_test = r2_score(target_test, prediction_test)\n",
    "    predicted_mean_test = np.mean(prediction_test)\n",
    "    predicted_std_test = np.std(prediction_test)\n",
    "    \n",
    "    r2_train_distribution, r2_test_distribution, mae_train_distribution, mae_test_distribution, _, _ = generate_score_distributions(model, atribute_train, target_train, atribute_test, target_test, bplot=False)   \n",
    "        \n",
    "    _, z_pval_r2 = get_ztest_2samples(r2_train_distribution, r2_test_distribution, bprint = False)\n",
    "    _, z_pval_mae = get_ztest_2samples(mae_train_distribution, mae_test_distribution, bprint = False)\n",
    "#     _, z_pval_mse = get_ztest_2samples(mse_train_distribution, mse_test_distribution, bprint = False)\n",
    "    _, t_pval_r2 = get_ttest_2samples(r2_train_distribution, r2_test_distribution, bprint = False)\n",
    "    _, t_pval_mae = get_ttest_2samples(mae_train_distribution, mae_test_distribution, bprint = False)\n",
    "#     _, t_pval_mse =get_ttest_2samples(mse_train_distribution, mse_test_distribution, bprint = False)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df = {'Model':model_name ,'Data': ddata, 'mse_train':mse_train, 'mae_train':mae_train, 'rmse_train':rmse_train, 'r2_train':r2_train, \n",
    "                       'predicted_train_mean': predicted_mean_train, 'predicted_train_std': predicted_std_train, 'mse_test':mse_test, 'mae_test':mae_test, \n",
    "                       'rmse_test':rmse_test, 'r2_test':r2_test, 'predicted_test_mean': predicted_mean_test, \n",
    "                       'predicted_test_std':predicted_std_test, 'pval_ztest_r2': z_pval_r2 , 'pval_ttest_r2': t_pval_r2, \n",
    "                       'pval_ztest_mae': z_pval_mae , 'pval_ttest_mae':t_pval_mae ,'model_filename': model_filename}\n",
    "    \n",
    "    \n",
    "    results = results.append(df, ignore_index = True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерира DataFrame с резултатите от текущо разглеждан модел\n",
    "def get_current_model_evaluation_report(df):\n",
    "    result = pd.DataFrame(columns=['Metric', 'Train', 'Test'])\n",
    "    result['Metric']  =  np.array(['Mean squared error', 'Mean absolute error', 'Root mean squared error', 'Coefficient of determination', 'Predicted mean', 'Predicted_std'])   \n",
    "    result['Train']  =  np.array(df.round(decimals = 4).T[2:8])  \n",
    "    result['Test']  =  np.array(df.round(decimals = 4).T[8:14])  \n",
    "    result = result.set_index('Metric')\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
